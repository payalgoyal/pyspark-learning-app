[
    {
        "level_id": 1,
        "topic" : "Filtering rows",
        "mission": "Filter out all rows where age is less than 25. Use 'result' to store the code",
        "dataset": "data/level_1_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level1_expected.csv",
        "hint": "Use the filter() function and compare the age column with 25.",
        "solution": "result = df.filter(df[\"age\"]>=25)",
        "group": "core",
        "position":1,
        "explain": "Filtering Rows: `filter()` is used to filter rows based on a given condition or expression."
    },
    {
        "level_id": 2,
        "topic" : "Selecting Columns",
        "mission": "Select only the name column from the dataset. Use 'result' to store the code",
        "dataset": "data/level_2_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level2_expected.csv",
        "hint": "Use the select() function to get the name from the dataset",
        "solution": "result = df.select(\"name\")",
        "group": "core",
        "position": 1,
        "explain": "Selecting Columns: `select()` is used to choose specific columns from a DataFrame."
    },
    {
        "level_id": 3,
        "topic" : "Creating or Updating Columns",
        "mission": "Add a new column named \"age_after_5_years\" which is age + 5. Use 'result' to store the code",
        "dataset": "data/level_3_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level3_expected.csv",
        "hint": "Use the withCoulmn() function and 'col' to identify the column name on which new column depends",
        "solution": "result = df.withColumn(\"age_after_5_years\", col(\"age\") + 5)",
        "group": "core",
        "position": 1,
        "explain": "Creating or Updating Columns: `withColumn()` is used to add a new column or modify an existing one using an expression. Existing column name should be within col()"
    },
    {
        "level_id": 4,
        "topic" : "Dropping Column",
        "mission": "Drop the city column from the dataset. Use 'result' to store the code",
        "dataset": "data/level_4_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level4_expected.csv",
        "hint": "Use the drop() function and pass the column name to drop",
        "solution": "result = df.drop(\"city\")",
        "group": "core",
        "position": 1,
        "explain": "Dropping Columns: `drop()` is used to remove one or more columns from a DataFrame."
    },
    {
        "level_id": 5,
        "topic" : "distinct()",
        "mission": "Get all distinct cities from the dataset. Use 'result' to store the code",
        "dataset": "data/level_5_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level5_expected.csv",
        "hint": "Use the distinct() function and pass the column name to get distinct value",
        "solution": "result = df.select(\"city\").distinct",
        "group": "core",
        "position": 1,
        "explain": "Removing Duplicate Rows: `distinct()` is used to return a new DataFrame with duplicate rows removed."
    },
    {
        "level_id": 6,
        "topic": "Aggregating Data",
        "mission": "Calculate the total sales for each city and give alias as total_sales. Store the result in 'result'.",
        "dataset": "data/level_6_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level6_expected.csv",
        "hint": "Use groupBy() along with agg() to perform aggregation. You can use sum() for total sales.",
        "solution": "from pyspark.sql.functions import sum\nresult = df.groupBy(\"city\").agg(sum(\"sales\").alias(\"total_sales\"))",
        "group": "aggregate",
        "position": 2,
        "explain": "Aggregating Data: `agg()` is used with `groupBy()` to apply aggregate functions like sum, avg, min, or max to grouped data."
    },
    {
        "level_id": 7,
        "topic": "Averaging Values",
        "mission": "Calculate the average sales for each city use alias as 'average_sales'. Store the result in 'result'.",
        "dataset": "data/level_7_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level7_expected.csv",
        "hint": "Use groupBy() with agg() and apply avg() to the 'sales' column.",
        "solution": "from pyspark.sql.functions import avg\nresult = df.groupBy(\"city\").agg(avg(\"sales\").alias(\"average_sales\"))",
        "group": "aggregate",
        "position": 2,
        "explain": "Averaging Values: `avg()` is used inside `agg()` to calculate the average of a numerical column for grouped data."
    },
    {
        "level_id": 8,
        "topic": "Counting Rows",
        "mission": "Count how many records exist for each city. Store the result in 'result'.",
        "dataset": "data/level_8_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level8_expected.csv",
        "hint": "Use groupBy() and count() to count rows for each group.",
        "solution": "result = df.groupBy(\"city\").count()",
        "group": "aggregate",
        "position": 2,
        "explain": "Counting Rows: `count()` is used to count the number of records in each group when used with `groupBy()`."
    },
    {
        "level_id": 9,
        "topic": "Finding Maximum Value",
        "mission": "Find the highest sales value for each city. Set alias as 'max_sales'. Store the result in 'result'.",
        "dataset": "data/level_9_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level9_expected.csv",
        "hint": "Use groupBy() and apply max() to the sales column.",
        "solution": "from pyspark.sql.functions import max\nresult = df.groupBy(\"city\").agg(max(\"sales\").alias(\"max_sales\"))",
        "group": "aggregate",
        "position": 2,
        "explain": "Finding Maximum Value: `max()` is used to get the highest value in a column, especially useful when grouping data."
    },
    {
        "level_id": 10,
        "topic": "Removing Duplicate Records",
        "mission": "Remove duplicate rows based on 'name' and 'city'. Store the result in 'result'.",
        "dataset": "data/level_10_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level10_expected.csv",
        "hint": "Use dropDuplicates() with a list of column names.",
        "solution": "result = df.dropDuplicates([\"name\", \"city\"])",
        "group": "core",
        "position": 1,
        "explain": "Removing Duplicate Records: `dropDuplicates()` removes rows that have the same values in specified columns."
    },
    {
        "level_id": 11,
        "topic": "String Conversion to Uppercase",
        "mission": "Convert the names to uppercase and store them in a new column called 'name_upper'.",
        "dataset": "data/level_11_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level11_expected.csv",
        "hint": "Use withColumn() and upper() from pyspark.sql.functions.",
        "solution": "from pyspark.sql.functions import upper\nresult = df.withColumn(\"name_upper\", upper(df[\"name\"]))",
        "group": "String Functions",
        "position": 3,
        "explain": "String Conversion to Uppercase: `upper()` converts all characters in a string column to uppercase."
    },
    {
        "level_id": 12,
        "topic": "Substring Extraction",
        "mission": "Extract the first 3 letters of each city and store in a new column 'city_short'.",
        "dataset": "data/level_12_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level12_expected.csv",
        "hint": "Use substr(column, start, length) from pyspark.sql.functions.",
        "solution": "from pyspark.sql.functions import col\nresult = df.withColumn(\"city_short\", col(\"city\").substr(1, 3))",
        "group": "String Functions",
        "position": 3,
        "explain": "Substring Extraction: `substr()` is used to extract a portion of a string based on starting position and length."
    },
    {
        "level_id": 13,
        "topic": "Column Concatenation",
        "mission": "Concatenate 'first_name' and 'last_name' into 'full_name'.",
        "dataset": "data/level_13_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level13_expected.csv",
        "hint": "Use concat() and add a space between names using lit().",
        "solution": "from pyspark.sql.functions import concat, lit\nresult = df.withColumn(\"full_name\", concat(df.first_name, lit(\" \"), df.last_name))",
        "group": "String Functions",
        "position": 3,
        "explain": "Column Concatenation: `concat()` is used to combine multiple columns or string literals into one column."
    },
    {
        "level_id": 14,
        "topic": "Delimited Concatenation",
        "mission": "Concatenate 'first_name' and 'last_name' with '-' in between.",
        "dataset": "data/level_14_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level14_expected.csv",
        "hint": "Use concat_ws('-', col1, col2) to add delimiter.",
        "solution": "from pyspark.sql.functions import concat_ws\nresult = df.withColumn(\"full_name\", concat_ws(\"-\", df.first_name, df.last_name))",
        "group": "String Functions",
        "position": 3,
        "explain": "Delimited Concatenation: `concat_ws()` joins columns into a single string with a specified separator."
    },
    {
        "level_id": 15,
        "topic": "Pattern Matching",
        "mission": "Extract domain name from email addresses.",
        "dataset": "data/level_15_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level15_expected.csv",
        "hint": "Use regexp_extract() with pattern '@(\\w+)' to extract domain.",
        "solution": "from pyspark.sql.functions import regexp_extract\nresult = df.withColumn(\"domain\", regexp_extract(df.email, \"@(\\\\w+)\", 1))",
        "group": "String Functions",
        "position": 3,
        "explain": "Pattern Matching: `regexp_extract()` extracts matching groups from strings using regular expressions."
    },
    {
        "level_id": 16,
        "topic": "Wildcard Matching",
        "mission": "Filter rows where city starts with 'Del'.",
        "dataset": "data/level_16_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level16_expected.csv",
        "hint": "Use filter() with col.like('Del%').",
        "solution": "result = df.filter(df.city.like(\"Del%\"))",
        "group": "String Functions",
        "position": 3,
        "explain": "Wildcard Matching: `like()` allows SQL-style pattern matching using '%' and '_' wildcards."
    },
    {
        "level_id": 17,
        "topic": "rlike()",
        "mission": "Filter names that contain numbers using regex. Filter name having digits in them",
        "dataset": "data/level_17_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level17_expected.csv",
        "hint": "Use rlike('[0-9]') to match digits.",
        "solution": "result = df.filter(df.name.rlike(\"[0-9]\"))",
        "group": "String Functions",
        "position": 3,
        "explain": "Regex Matching: `rlike()` filters rows where a column matches a regular expression."
    },
    {
        "level_id": 18,
        "topic": "Prefix Check",
        "mission": "Filter names that start with 'A'.",
        "dataset": "data/level_18_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level18_expected.csv",
        "hint": "Use startswith('A') inside filter.",
        "solution": "result = df.filter(df.name.startswith(\"A\"))",
        "group": "String Functions",
        "position": 3,
        "explain": "Prefix Check: `startswith()` checks if a string column begins with a specific value."
    },
    {
        "level_id": 19,
        "topic": "Suffix Check",
        "mission": "Filter cities that end with 'pur'.",
        "dataset": "data/level_19_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level19_expected.csv",
        "hint": "Use endswith('pur') in a filter expression.",
        "solution": "result = df.filter(df.city.endswith(\"pur\"))",
        "group": "String Functions",
        "position": 3,
        "explain": "Suffix Check: `endswith()` returns True if the string column ends with the specified substring."
    },
    {
        "level_id": 20,
        "topic": "Null Handling",
        "mission": "Drop all rows that contain any null values.",
        "dataset": "data/level_20_data.csv",
        "dataset_name" : ["DataSet"],
        "expected_result": "expected_results/level20_expected.csv",
        "hint": "Use df.na.drop() to remove rows with nulls.",
        "solution": "result = df.na.drop()",
        "group": "core",
        "position": 1,
        "explain": "Null Handling: `na.drop()` removes rows with missing or null values from a DataFrame."
    },
    {
        "level_id": 21,
        "topic": "Merging Data",
        "mission": "Join customer data with order data on 'customer_id'.",
        "dataset": "data/level_21_customer.csv, data/level_21_orders.csv",
        "dataset_name" : ["Customer","Orders"],
        "expected_result": "expected_results/level21_expected.csv",
        "hint": "Use join() and specify join key and type.",
        "solution": "result = df1.join(df2, on=\"customer_id\", how=\"inner\")",
        "group": "core",
        "position": 1,
        "explain": "Merging Data: `join()` combines rows from two DataFrames based on a common column."
    }

]